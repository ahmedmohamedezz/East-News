{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import os\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt', quiet=True)\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/labeled_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(df.head())\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df['text']\n",
    "y = df['class']  # Assuming 'class' column contains the labels 0, 1, 2 for hate speech, offensive language, neither\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize NLTK resources for text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in string.punctuation]  # Remove stopwords and punctuation, and lemmatize\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the training and testing sets\n",
    "X_train_processed = X_train.apply(preprocess_text)\n",
    "X_test_processed = X_test.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1004)  # Adjust max_features as needed\n",
    "\n",
    "# Fit vectorizer and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
    "\n",
    "# Transform testing data (using the same vectorizer fitted on training data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression model\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = lr_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(comment, model, vectorizer):\n",
    "    # Preprocess the comment\n",
    "    processed_comment = preprocess_text(comment)\n",
    "    # Vectorize the comment\n",
    "    vectorized_comment = vectorizer.transform([processed_comment])\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(vectorized_comment)\n",
    "    return prediction[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_file = '/content/drive/MyDrive/logistic_regression_model.pkl'\n",
    "vectorizer_file = '/content/drive/MyDrive/tfidf_vectorizer.pkl'\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(lr_classifier, '/content/drive/MyDrive/model.joblib')\n",
    "joblib.dump(tfidf_vectorizer, '/content/drive/MyDrive/tfidf_vectorizer.pkl')\n",
    "\n",
    "# Assuming X_train_tfidf is your sparse training data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train_tfidf)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, '/content/drive/MyDrive/ scaler.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
